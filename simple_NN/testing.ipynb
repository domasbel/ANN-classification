{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0, 0],\n",
      "          [0, 0]],\n",
      "\n",
      "         [[0, 0],\n",
      "          [0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0],\n",
      "          [0, 0]],\n",
      "\n",
      "         [[0, 0],\n",
      "          [0, 0]]]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# viable examples is rand, empty, zeros\n",
    "# also different kind of dtypes can be applied for a torch\n",
    "\n",
    "x = torch.zeros(2, 2, 2, 2, dtype=torch.int)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.3000, 0.1000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.3 , 0.1])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7715, 0.3189],\n",
      "        [0.1398, 0.4551]])\n",
      "tensor([[0.8221, 0.1515],\n",
      "        [0.3794, 0.1570]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 2)\n",
    "y = torch.rand(2, 2)\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5936, 0.4704],\n",
      "        [0.5192, 0.6121]])\n"
     ]
    }
   ],
   "source": [
    "z = x + y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5936, 0.4704],\n",
      "        [0.5192, 0.6121]])\n"
     ]
    }
   ],
   "source": [
    "# all of the functions with _ does the operation for specific element\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9563, 0.3391, 0.7982],\n",
      "        [0.3381, 0.8495, 0.4383],\n",
      "        [0.3808, 0.7480, 0.3231],\n",
      "        [0.9498, 0.4871, 0.8331],\n",
      "        [0.5974, 0.1904, 0.9855]])\n",
      "tensor([0.9563, 0.3381, 0.3808, 0.9498, 0.5974])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)\n",
    "print(x[:, 0]) # for only the first colum, supports the same manipulation as arrays in np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5544, 0.0409],\n",
      "        [0.6090, 0.4307],\n",
      "        [0.2884, 0.6987]])\n",
      "tensor([[0.5544, 0.0409, 0.6090],\n",
      "        [0.4307, 0.2884, 0.6987]])\n"
     ]
    }
   ],
   "source": [
    "# reshaping\n",
    "\n",
    "x = torch.rand(3, 2)\n",
    "print(x)\n",
    "y = x.view(2, 3)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.]\n",
      "tensor([0., 0.], dtype=torch.float64)\n",
      "[2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros(2)\n",
    "print(a)\n",
    "b = torch.from_numpy(a)\n",
    "print(b)\n",
    "\n",
    "b.add_(2)\n",
    "print(a)\n",
    "# here we can see they share the same memory storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    x = torch.ones(5, device=device)\n",
    "    y = torch.ones(4)\n",
    "    y = y.to(device=device)\n",
    "    z = x + y # this will be performed on gpu if its present\n",
    "    z = z.to('cpu') # this is beacuse transfering to numpy is available only via cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5, requires_grad=True)\n",
    "# this by default is False, but if you know that this tensor will have its gradeient calculated later on, so for optimization it should be set as true\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2058, -1.6199, -0.3467], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# this grad variable is important if we want to calculate the backprop and if it is not present grad wont work\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.7942, 0.3801, 1.6533], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.4385, 0.2890, 5.4666], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 2\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.0647, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = z.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.1100, 2.0274, 2.2110])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.001], dtype=torch.float32)\n",
    "z.backward(v) # this is the jacobian product\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires grad can be removed at all using\n",
    "# x.requires_grad_\n",
    "# x.detach()\n",
    "# with torch.no_grad():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model_output = (weights*3).sum()\n",
    "    model_output.backward() # here we calculate the gradients\n",
    "    print(weights.grad)\n",
    "\n",
    "    weights.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients yra daliniu isvestiniu vektoriai ir apibreziai kaip funkcija pasikeicia su tam tikrais inputais, ar tai tam tikri features ar tai tam tikri skaicia, kurie ateina kaip input, taip parodant kaip svoriai turi pasikeisti, jog sumazetu loss funkcijos reiksme. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# forward pass and compute the loss\n",
    "\n",
    "y_hat = w * x\n",
    "loss = (y_hat - y)**2\n",
    "print(loss)\n",
    "\n",
    "# backward pass\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "\n",
    "# update our wieghts\n",
    "# next forward and backwards pass for several iteration to optimize wieghts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# everything from scarth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 1.200, loss = -10.00000000\n",
      "epoch 3: w = 1.872, loss = -1.60000014\n",
      "epoch 5: w = 1.980, loss = -0.25600022\n",
      "epoch 7: w = 1.997, loss = -0.04096031\n",
      "epoch 9: w = 1.999, loss = -0.00655347\n",
      "epoch 11: w = 2.000, loss = -0.00104839\n",
      "epoch 13: w = 2.000, loss = -0.00016820\n",
      "epoch 15: w = 2.000, loss = -0.00002688\n",
      "epoch 17: w = 2.000, loss = -0.00000411\n",
      "epoch 19: w = 2.000, loss = -0.00000066\n",
      "prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# formula is 2 * x\n",
    "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
    "\n",
    "w = 0.0\n",
    "\n",
    "# model prediction calculation\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y) * 2).mean()\n",
    "\n",
    "# gradients\n",
    "# MSE = 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N 2x (w*x - y)\n",
    "\n",
    "def gardient(x, y, y_pred):\n",
    "    return np.dot(2*x, y_pred-y).mean()\n",
    "\n",
    "print(f'prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# training\n",
    "learning_rate = 0.01\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # pred = foward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # gradients\n",
    "    dw = gardient(X, Y, y_pred)\n",
    "\n",
    "    # w updates\n",
    "    w -= learning_rate * dw\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'prediction after training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 6: w = 1.246, loss = 5.90623236\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 16: w = 1.851, loss = 0.22892261\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 26: w = 1.971, loss = 0.00887291\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 36: w = 1.994, loss = 0.00034392\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 46: w = 1.999, loss = 0.00001333\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 56: w = 2.000, loss = 0.00000052\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 66: w = 2.000, loss = 0.00000002\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 76: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 86: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "epoch 96: w = 2.000, loss = 0.00000000\n",
      "prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# after the manual calculation we can transfer it to torch\n",
    "\n",
    "\n",
    "# formula is 2 * x\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# model prediction calculation\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y) ** 2).mean()\n",
    "\n",
    "# gradients\n",
    "# MSE = 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N 2x (w*x - y)\n",
    "\n",
    "def gardient(x, y, y_pred):\n",
    "    return np.dot(2*x, y_pred-y).mean()\n",
    "\n",
    "print(f'prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# training\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # pred = foward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # gradients\n",
    "    l.backward() # this will calculate the gradient to loss\n",
    "\n",
    "    # w updates\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "\n",
    "    # we must empty the gradients, so that we wouldnt accumulate them\n",
    "    w.grad.zero_()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'prediction after training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after adjusting gradient computations to autograd, next we will adjust loss function and parameter updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before training: f(5) = -0.188\n",
      "epoch 1: w = 2.000, loss = 31.66109276\n",
      "epoch 21: w = 2.000, loss = 0.05807181\n",
      "epoch 41: w = 2.000, loss = 0.03274921\n",
      "epoch 61: w = 2.000, loss = 0.02903523\n",
      "epoch 81: w = 2.000, loss = 0.02575358\n",
      "epoch 101: w = 2.000, loss = 0.02284284\n",
      "epoch 121: w = 2.000, loss = 0.02026111\n",
      "epoch 141: w = 2.000, loss = 0.01797112\n",
      "epoch 161: w = 2.000, loss = 0.01593998\n",
      "epoch 181: w = 2.000, loss = 0.01413840\n",
      "prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# formula is 2 * x\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "x_test = torch.tensor([[5]], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "#model = nn.Linear(input_size, output_size)\n",
    "\n",
    "class LinearReggression(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearReggression, self).__init__()\n",
    "        # define layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "\n",
    "model = LinearReggression(input_size, output_size)\n",
    "\n",
    "print(f'prediction before training: f(5) = {model(x_test).item():.3f}')\n",
    "\n",
    "# training\n",
    "learning_rate = 0.01\n",
    "epochs = 200\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # pred = foward pass\n",
    "    y_pred = model(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # gradients\n",
    "    l.backward() # this will calculate the gradient to loss\n",
    "\n",
    "    # w updates\n",
    "    optimizer.step()\n",
    "\n",
    "    # we must empty the gradients, so that we wouldnt accumulate them\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'prediction after training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
